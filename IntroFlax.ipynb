{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    "frame\n",
    "1. autograd frame: jax, jax.numpy\n",
    "2. neural network frame: flax\n",
    "3. optimizer frame: optax\n",
    "\n",
    "requrement\n",
    "- [x] complex net: conv1d, GRUCell, FCN, GRU net\n",
    "- [ ] complex actvation function.\n",
    "- [x] complex optimizer: take real parameter !!\n",
    "torch support: sgd, adam\n",
    "optax support: None\n",
    "\n",
    "some operations from pytorch to jax.numpy\n",
    "1. jnp.transpose(x,[0,1,2]): permute the axes of an array\n",
    "2. jnp.expand_dims(x, axis): add a dim to x\n",
    "3. jnp.repeat: repeat a dim\n",
    "4. jnp.squeeze: Remove axes of length one from a.\n",
    "5. jnp.concatenate\n",
    "6. jnp.stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Static Module: optimize with complex parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6720802\n",
      "3.6354813e-06\n",
      "4.754816e-11\n",
      "2.654127e-15\n",
      "1.3877788e-17\n",
      "1.3877788e-17\n",
      "1.3877788e-17\n",
      "1.3877788e-17\n",
      "1.3877788e-17\n",
      "1.3877788e-17\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import device_put, device_get\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from functools import partial, wraps\n",
    "from typing import Any, NamedTuple\n",
    "from optical_flax.utils import realize, normal_init, show_tree, tree_r2c, tree_c2r\n",
    "from collections import namedtuple\n",
    "from flax.core import freeze, unfreeze\n",
    "\n",
    "# 6. 复数优化代码: static module\n",
    "class NN(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs  # NWC\n",
    "        conv = partial(nn.Conv,features=1,kernel_size=(3,), param_dtype=jnp.complex64, dtype=jnp.complex64)\n",
    "        x = conv()(x) # NWC\n",
    "        x = nn.relu(x.real) + nn.relu(x.imag) * (1j) # NWC\n",
    "        x = nn.DenseGeneral(features=1, param_dtype=jnp.complex64, dtype=jnp.complex64, axis=(-2,-1))(x) # N1\n",
    "        return x\n",
    "\n",
    "net_org = NN()\n",
    "net = realize(net_org)\n",
    "\n",
    "N,W,C = 5,1000,2\n",
    "x0 = np.random.rand(N,W,C) + 1j * np.random.rand(N,W,C)\n",
    "x0 = device_put(x0)\n",
    "\n",
    "param_true = net.init(random.PRNGKey(123), x0)\n",
    "y0 = net.apply(param_true, x0)\n",
    "param = net.init(random.PRNGKey(234), x0)\n",
    "\n",
    "from commplax import optim\n",
    "f = optim.piecewise_constant([500, 1000], [2e-3, 1e-4, 1e-5])\n",
    "tx = optax.adam(learning_rate=f)\n",
    "opt_state = tx.init(param)\n",
    "\n",
    "@jax.jit\n",
    "def MSE(f_hat, f):\n",
    "    return jnp.sum(jnp.abs(f_hat - f)**2)\n",
    "\n",
    "@jax.jit\n",
    "def loss(params,x0,y0):\n",
    "    y1 = net.apply(params, x0)\n",
    "    return MSE(y1,y0)\n",
    "\n",
    "loss_grad_fn = jax.jit(jax.value_and_grad(loss))\n",
    "\n",
    "State = namedtuple('TrainState', ['params','opt_state'])\n",
    "Data = namedtuple('Data', ['x','y'])\n",
    "data = Data(x=x0,y=y0)\n",
    "\n",
    "@jax.jit\n",
    "def update_state(state, data):\n",
    "    loss_val, grads = loss_grad_fn(state.params, data.x, data.y)\n",
    "    updates,opt_state = tx.update(grads, state.opt_state)\n",
    "    params = optax.apply_updates(state.params, updates)\n",
    "    return State(params=params, opt_state=opt_state), loss_val\n",
    "    \n",
    "## training 不用 scan\n",
    "loss_list = []\n",
    "state = State(params=param, opt_state=opt_state)\n",
    "for i in range(1000):\n",
    "    state, loss_val = update_state(state, data)\n",
    "    loss_list.append(loss_val)\n",
    "    if i % 100 == 0:\n",
    "        print(loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Dynamic module: applying and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeADF(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self,inputs):\n",
    "        ## 探索variable的更新机制\n",
    "        x = inputs\n",
    "\n",
    "        # 定义中间变量\n",
    "        running_mean = self.variable('af','mean',lambda s: jnp.ones(s), (1,))\n",
    "\n",
    "        # 更新中间变量\n",
    "        mean = jnp.sum(x**2)\n",
    "        running_mean.value = running_mean.value * 0.99 + mean * (1 - 0.99)\n",
    "\n",
    "        # 返回最终值\n",
    "        return x / jnp.sqrt(running_mean.value)\n",
    "\n",
    "class LeDSP(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs   # NWC\n",
    "        x = nn.Dense(features=5)(x)  # N1\n",
    "        x = LeADF()(x)\n",
    "        return x\n",
    "\n",
    "def test_net(net, mutable='af', train_key='params'):\n",
    "    N,W,C = 5,1000,2\n",
    "    x0 = np.random.rand(N,W,C) + 1j * np.random.rand(N,W,C)\n",
    "    x0 = device_put(x0)\n",
    "\n",
    "    var_true = net.init(random.PRNGKey(123), x0)\n",
    "    y0, real_af_state = net.apply(var_true, x0, mutable=mutable)\n",
    "    var = net.init(random.PRNGKey(234), x0)\n",
    "\n",
    "\n",
    "    tx = optax.adam(learning_rate=0.001)\n",
    "    opt_state = tx.init(var[train_key])\n",
    "\n",
    "\n",
    "    @jax.jit\n",
    "    def MSE(f_hat, f):\n",
    "        return jnp.sum(jnp.abs(f_hat - f)**2)\n",
    "\n",
    "    @jax.jit\n",
    "    def loss(params,af_state,x0,y0):\n",
    "        var = freeze({train_key:params, mutable: af_state})\n",
    "        y1, new_af_state = net.apply(var, x0, mutable=mutable)\n",
    "        return MSE(y1,y0), new_af_state\n",
    "\n",
    "    loss_grad_fn = jax.jit(jax.value_and_grad(loss, has_aux=True))\n",
    "\n",
    "    State = namedtuple('TrainState', ['params','af_state','opt_state'])\n",
    "    Data = namedtuple('Data', ['x','y'])\n",
    "    data = Data(x=x0,y=y0)\n",
    "\n",
    "    @partial(jax.jit,static_argnums=(2))\n",
    "    def update_state(state, data, tx):\n",
    "        (loss_val,new_af_state), grads = loss_grad_fn(state.params,state.af_state, data.x, data.y)\n",
    "        updates,opt_state = tx.update(grads, state.opt_state)\n",
    "        params = optax.apply_updates(state.params, updates)\n",
    "        return State(params=params,af_state=new_af_state[mutable], opt_state=opt_state), loss_val\n",
    "        \n",
    "    ## training 不用 scan\n",
    "    loss_list = []\n",
    "    state = State(params=var[train_key],af_state=var[mutable], opt_state=opt_state)\n",
    "    for i in range(1000):\n",
    "        state, loss_val = update_state(state, data, tx)\n",
    "        loss_list.append(loss_val)\n",
    "        if i % 100 == 0:\n",
    "            print(loss_val)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    af: {\n",
       "        LeADF_0: {\n",
       "            mean: (1,),\n",
       "        },\n",
       "    },\n",
       "    params: {\n",
       "        Dense_0: {\n",
       "            bias: (5,),\n",
       "            kernel: (10, 5),\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(123)\n",
    "net = LeDSP()\n",
    "show_tree(net.init(key, jnp.ones([10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    af: {\n",
       "        LeADF_0: {\n",
       "            mean: (5, 1),\n",
       "        },\n",
       "    },\n",
       "    params: {\n",
       "        Dense_0: {\n",
       "            bias: (5,),\n",
       "            kernel: (10, 5),\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DSP_vmap = nn.vmap(LeDSP, variable_axes={'params':None, 'af':0}, split_rngs={'params':False, 'af':False}, in_axes=0,out_axes=0)\n",
    "net = DSP_vmap()\n",
    "var = net.init(key, jnp.ones([5,10]))\n",
    "show_tree(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[-0.56203514,  0.6615603 , -0.06052642,  1.2041702 ,\n",
       "               -1.4184822 ],\n",
       "              [-0.56203514,  0.6615603 , -0.06052642,  1.2041702 ,\n",
       "               -1.4184822 ],\n",
       "              [-0.56203514,  0.6615603 , -0.06052642,  1.2041702 ,\n",
       "               -1.4184822 ],\n",
       "              [-0.56203514,  0.6615603 , -0.06052642,  1.2041702 ,\n",
       "               -1.4184822 ],\n",
       "              [-0.56203514,  0.6615603 , -0.06052642,  1.2041702 ,\n",
       "               -1.4184822 ]], dtype=float32),\n",
       " FrozenDict({\n",
       "     af: {\n",
       "         LeADF_0: {\n",
       "             mean: DeviceArray([[1.0699368],\n",
       "                          [1.0699368],\n",
       "                          [1.0699368],\n",
       "                          [1.0699368],\n",
       "                          [1.0699368]], dtype=float32),\n",
       "         },\n",
       "     },\n",
       " }))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.apply(var, jnp.ones([5,10]), mutable='af')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinyu/opt/anaconda3/envs/commplax/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:3673: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  out = lax._convert_element_type(out, dtype, weak_type=weak_type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131.25679\n",
      "54.10591\n",
      "51.016037\n",
      "48.87536\n",
      "47.155968\n",
      "45.792076\n",
      "44.73971\n",
      "43.941395\n",
      "43.338417\n",
      "42.880535\n"
     ]
    }
   ],
   "source": [
    "## dynamic module\n",
    "net_org = LeDSP()\n",
    "net = realize(net_org)\n",
    "state = test_net(net)\n",
    "#var = net.init(random.PRNGKey(0),x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Use the NN internal module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 利用NN内部模块\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "  def setup(self):\n",
    "    self.encoder = nn.Dense(3)\n",
    "    self.decoder = nn.Dense(5)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return self.decoder(self.encoder(x))\n",
    "\n",
    "x = jnp.ones((16, 9))\n",
    "ae = AutoEncoder()\n",
    "variables = ae.init(jax.random.PRNGKey(0), x)\n",
    "model = ae.bind(variables)\n",
    "z = model.encoder(x)\n",
    "x_reconstructed = model.decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 100, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Dense layer: act on the last dimension of x\n",
    "key = random.PRNGKey(0)\n",
    "net = nn.Dense(features=1, kernel_init=normal_init, bias_init=normal_init, param_dtype=jnp.complex64, dtype=jnp.complex64)\n",
    "x = jnp.ones([5,100,3])+1j\n",
    "params = net.init(key, x)\n",
    "net.apply(params, x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-DenseGeneral layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3, 4, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## DenseGeneral: flexible axies\n",
    "# 会把作用的axis移到最后一个轴\n",
    "net = nn.DenseGeneral(features=1, kernel_init=normal_init, bias_init=normal_init,param_dtype=jnp.complex64, dtype=jnp.complex64, axis=(1))\n",
    "x = jnp.ones([5,100,3,4])\n",
    "params = net.init(key, x)\n",
    "net.apply(params, x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-Conv Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## conv layer: input: NWC\n",
    "net = nn.Conv(features=2,kernel_size=(2,),strides=(2,),kernel_init=normal_init, param_dtype=jnp.complex64,dtype=jnp.complex64, padding='same')\n",
    "x = jnp.ones([100,2])\n",
    "params = net.init(key, x)\n",
    "net.apply(params, x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-GRU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5) (10, 5)\n",
      "out_carry shape: (16, 5), out_val shape (16, 20, 5)\n",
      "init_carry shape: (16, 5)\n"
     ]
    }
   ],
   "source": [
    "## GRUCell\n",
    "net = nn.GRUCell()\n",
    "params = net.init(key, jnp.ones([10,5]), jnp.ones([10,11]))\n",
    "carry, output = net.apply(params, jnp.ones([10,5]), jnp.ones([10,11]))\n",
    "print(carry.shape, output.shape)\n",
    "\n",
    "## GRU\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "from jax import random\n",
    "\n",
    "class GRU(nn.Module):\n",
    "  param_dtype:Any=jnp.float32\n",
    "  dtype:Any=jnp.float32\n",
    "  kernel_init: Any=normal_init\n",
    "  bias_init:Any=normal_init\n",
    "  @nn.compact\n",
    "  def __call__(self, c, xs):\n",
    "    NN = nn.scan(nn.GRUCell,\n",
    "                   variable_broadcast=\"params\",\n",
    "                   split_rngs={\"params\": False},\n",
    "                   in_axes=1,   # along axis=1 scan\n",
    "                   out_axes=1)  # output on axis=1\n",
    "    return NN(param_dtype=self.param_dtype, dtype=self.dtype, kernel_init=self.kernel_init, bias_init=self.bias_init)(c, xs)\n",
    "\n",
    "seq_len, batch_size, in_feat, out_feat = 20, 16, 3, 5\n",
    "key_1, key_2, key_3 = random.split(random.PRNGKey(0), 3)\n",
    "\n",
    "xs = random.uniform(key_1, (batch_size, seq_len, in_feat))\n",
    "init_carry = nn.GRUCell.initialize_carry(key_2, (batch_size,), out_feat).astype(jnp.complex64)\n",
    "\n",
    "model = GRU(param_dtype=jnp.complex64, dtype=jnp.complex64)\n",
    "variables = model.init(key_3, init_carry, xs)\n",
    "out_carry, out_val = model.apply(variables, init_carry, xs)\n",
    "\n",
    "assert out_val.shape == (batch_size, seq_len, out_feat)\n",
    "print(f'out_carry shape: {out_carry.shape}, out_val shape {out_val.shape}')\n",
    "print(f'init_carry shape: {init_carry.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8-Dense net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from optical_flax.functions import cleaky_relu, crelu\n",
    "from commplax.module.core import SigTime, Signal, zeros\n",
    "class Dense_net(nn.Module):\n",
    "    '''\n",
    "    Fully Connected network\n",
    "    '''\n",
    "    features:int = 20\n",
    "    width:list = (20,20,20)\n",
    "    dtype:Any = jnp.complex64\n",
    "    param_dtype:Any = jnp.complex64\n",
    "    act:Callable=crelu\n",
    "    bias_init:Callable=zeros\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,inputs):\n",
    "        Dense  = partial(nn.Dense, dtype=self.dtype, param_dtype=self.param_dtype)\n",
    "        x = inputs\n",
    "        for w in self.width:\n",
    "            x = Dense(features=w,bias_init=zeros)(x)\n",
    "            x = self.act(x)\n",
    "        x = Dense(features=self.features, bias_init=self.bias_init)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-nn.vmap on static module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_vmap_x(module):\n",
    "    return nn.vmap(module, \n",
    "    variable_axes={'params':-1},  # 表示变量'params'会沿着axis=-1复制, 'const'不会复制\n",
    "    split_rngs={'params':True}, # 表示初始化的种子会split\n",
    "    in_axes=-1, out_axes=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[1., 1.],\n",
       "             [1., 1.],\n",
       "             [1., 1.],\n",
       "             [1., 1.],\n",
       "             [1., 1.],\n",
       "             [1., 1.],\n",
       "             [1., 1.],\n",
       "             [1., 1.],\n",
       "             [1., 1.],\n",
       "             [1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FDBP Module\n",
    "class myFDBP(nn.Module):\n",
    "    # 定义初始化参数的函数\n",
    "    dtype: Any=jnp.complex64\n",
    "    weight_init: Callable=normal_init\n",
    "    bias_init: Callable=normal_init\n",
    "    steps: int = 3\n",
    "    dtaps: int = 261\n",
    "    ntaps: int = 41\n",
    "\n",
    "    def setup(self):\n",
    "        conv_c = partial(nn.Conv, use_bias=False, dtype=jnp.complex64, param_dtype=jnp.complex64, kernel_init=self.weight_init)\n",
    "        conv_r = partial(nn.Conv, use_bias=False, dtype=jnp.float32, param_dtype=jnp.float32, kernel_init=self.weight_init)\n",
    "        self.dconv_x = [conv_c(features=1, kernel_size=(self.dtaps,)) for i in range(self.steps)]\n",
    "        self.dconv_y = [conv_c(features=1, kernel_size=(self.dtaps,)) for i in range(self.steps)]\n",
    "        self.nconv = [conv_r(features=2, kernel_size=(self.ntaps,)) for i in range(self.steps)]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,inputs):\n",
    "        p = inputs\n",
    "\n",
    "        # 定义模型参数\n",
    "        for i in range(self.steps):\n",
    "            x = p[:,0:1]\n",
    "            y = p[:,1:2]\n",
    "            x = self.dconv_x[i](x)\n",
    "            y = self.dconv_y[i](y)\n",
    "            p = jnp.concatenate([x,y],axis=1)\n",
    "            arg = self.nconv[i](jnp.abs(p)**2)\n",
    "            p = p * jnp.exp(1j*(arg))\n",
    "            \n",
    "\n",
    "        # 运算与返回\n",
    "        return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-test long GRU (batch training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "918.75\n",
      "264.43524\n",
      "46.49644\n",
      "22.716423\n",
      "12.644978\n",
      "6.7692127\n",
      "3.3441296\n",
      "1.6718155\n",
      "1.3218107\n",
      "1.0349112\n",
      "0.8018294\n",
      "0.6996411\n",
      "0.65344936\n",
      "0.6236384\n",
      "0.6061782\n",
      "0.59661376\n",
      "0.5897596\n",
      "0.5848503\n",
      "0.58447164\n",
      "0.5967593\n"
     ]
    }
   ],
   "source": [
    "## test long GRU\n",
    "from optical_flax.layers import GRU\n",
    "seed = random.PRNGKey\n",
    "\n",
    "net = realize(GRU())\n",
    "L, B, in_feat, out_feat = 10000, 5, 3, 2\n",
    "key_1, key_2, key_3 = random.split(random.PRNGKey(0), 3)\n",
    "\n",
    "xs = random.uniform(key_1, (B,L,in_feat))\n",
    "init_carry = nn.GRUCell.initialize_carry(key_2, (B,), out_feat).astype(jnp.complex64)\n",
    "var_true = net.init(key_3, init_carry, xs)\n",
    "out_carry,ys = net.apply(var_true, init_carry, xs)\n",
    "var = net.init(seed(10), init_carry, xs)\n",
    "\n",
    "tx = optax.adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "def loss_fn(var, carry, xs, ys):\n",
    "    out_carry, y = net.apply(var, carry, xs)\n",
    "    return jnp.sum(jnp.abs(y - ys)**2), out_carry\n",
    "\n",
    "from jax.lax import stop_gradient \n",
    "\n",
    "\n",
    "## 由于jax没有计算图的概念，所以在这里并不需要 stop_gradient\n",
    "@jax.jit\n",
    "def update_step(var,opt_state, carry, xs, ys):\n",
    "    (loss_val, out_carry), grads = jax.value_and_grad(loss_fn, has_aux=True)(var, carry, xs, ys)\n",
    "    updates,opt_state = tx.update(grads, opt_state)\n",
    "    var = optax.apply_updates(var, updates)\n",
    "    return var, opt_state, stop_gradient(out_carry), loss_val\n",
    "\n",
    "\n",
    "carry = init_carry\n",
    "opt_state = tx.init(var)\n",
    "epochs = 200\n",
    "for j in range(epochs):\n",
    "    for i in range(100):\n",
    "        x = xs[:,i*100:(i+1)*100,:]\n",
    "        y = ys[:,i*100:(i+1)*100,:]\n",
    "        var, opt_state,carry,l  = update_step(var, opt_state, carry, x, y)\n",
    "    if j % 10==0:\n",
    "        print(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11-dispersion perator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Data would be auto-cached in default temporary location: /var/folders/lx/pn32t35562q_d2fbchp2r8wm0000gn/T/labptptm2, set labptptm2.config.cache_storage to other locations to suppress this warning\n",
      "                                                           \r"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import device_put, device_get\n",
    "import jax.random as random\n",
    "import numpy as np\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from functools import partial, wraps\n",
    "from typing import Any, NamedTuple,Callable, Iterable, Optional, Tuple, Union,Sequence\n",
    "from optical_flax.utils import realize, normal_init, show_tree, tree_r2c, tree_c2r\n",
    "from collections import namedtuple\n",
    "from flax.core import freeze, unfreeze\n",
    "from commplax.module import core\n",
    "from commplax.module.core import SigTime, Signal, zeros, conv1d_t, vmap,wpartial,delta,gauss\n",
    "import optical_flax.base as base\n",
    "import optical_flax.layers as layers\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "## Define datax\n",
    "from gdbp import gdbp_base as gb, data as gdat, aux\n",
    "data_train = gdat.load(1, 0, 4, 2)[0]\n",
    "data_test = gdat.load(2, 0, 4, 2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8e6aa48580>]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## dispersion operator\n",
    "a = data_train.a\n",
    "pi  = np.pi\n",
    "log = np.log\n",
    "exp = np.exp\n",
    "ifft = np.fft.ifft\n",
    "\n",
    "# virtual span is used in cases where we do not use physical span settings\n",
    "freqs = 1001\n",
    "sample_rate = a['samplerate']\n",
    "span_length = a['distance'] / a['spans']\n",
    "spans = a['spans']\n",
    "launch_power = a['lpdbm'] - 3   # launch power [dBm]\n",
    "                             \n",
    "steps_per_span=1                                 # steps per span\n",
    "virtual_spans=3                                  # number of virtual spans\n",
    "carrier_frequency=299792458/1550E-9              # carrier frequency [Hz]\n",
    "fiber_dispersion=16.5E-6                         # [s/m^2]\n",
    "fiber_dispersion_slope=0.08e3                    # [s/m^3]\n",
    "fiber_loss=.2E-3                                 # loss of fiber [dB/m]\n",
    "fiber_core_area=80E-12                           # effective area of fiber [m^2]\n",
    "fiber_nonlinear_index=2.6E-20                    # nonlinear index [m^2/W]\n",
    "fiber_reference_frequency=299792458/1550E-9      # fiber reference frequency [Hz]\n",
    "ignore_beta3=True\n",
    "polmux=True\n",
    "\n",
    "C       = 299792458. # speed of light [m/s]\n",
    "lambda_ = C / fiber_reference_frequency\n",
    "B_2     = -fiber_dispersion * lambda_**2 / (2 * pi * C)\n",
    "B_3     = 0. if ignore_beta3 else \\\n",
    "    (fiber_dispersion_slope * lambda_**2 + 2 * fiber_dispersion * lambda_) * (lambda_ / (2 * pi * C))**2 #[/m/W]\n",
    "gamma   = 2 * pi * fiber_nonlinear_index / lambda_ / fiber_core_area  #[/m/W]\n",
    "LP      = 10.**(launch_power / 10 - 3)  # 将【dBm】转化为 [W]\n",
    "alpha   = fiber_loss / (10. / log(10.)) # 计算出方程中的衰减系数alpha，z单位 [m]\n",
    "L_eff   = lambda h: (1 - exp(-alpha * h)) / alpha\n",
    "NIter   = virtual_spans * steps_per_span\n",
    "delay   = (freqs - 1) // 2\n",
    "dw      = 2 * pi * (carrier_frequency - fiber_reference_frequency)\n",
    "w_res   = 2 * pi * sample_rate / freqs\n",
    "k       = np.arange(freqs)\n",
    "w       = np.where(k > delay, k - freqs, k) * w_res # ifftshifted\n",
    "\n",
    "H   = exp(-1j * (-B_2 / 2 * (w + dw)**2 + B_3 / 6 * (w + dw)**3) * \\\n",
    "            span_length * spans / virtual_spans / steps_per_span)\n",
    "            ## dz = span_length * spans / virtual_spans / steps_per_span\n",
    "H_casual = H * exp(-1j * w * delay / sample_rate) ## 频域相位旋转等价于时域平移，将时域对齐\n",
    "h_casual = ifft(H_casual)\n",
    "\n",
    "if polmux:\n",
    "        dims = 2\n",
    "else:\n",
    "    dims = 1\n",
    "\n",
    "H = np.tile(H[None, :, None], (NIter, 1, dims))\n",
    "h_casual = np.tile(h_casual[None, :, None], (NIter, 1, dims))\n",
    "\n",
    "plt.plot(np.abs(h_casual[2,:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12-kalman filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KalmanFilter(object):\n",
    "    def __init__(self, F = None, B = None, H = None, Q = None, R = None, P = None, x0 = None):\n",
    "\n",
    "        if(F is None or H is None):\n",
    "            raise ValueError(\"Set proper system dynamics.\")\n",
    "\n",
    "        self.n = F.shape[1]\n",
    "        self.m = H.shape[1]\n",
    "\n",
    "        self.F = F\n",
    "        self.H = H\n",
    "        self.B = 0 if B is None else B\n",
    "        self.Q = np.eye(self.n) if Q is None else Q\n",
    "        self.R = np.eye(self.n) if R is None else R\n",
    "        self.P = np.eye(self.n) if P is None else P\n",
    "        self.x = np.zeros((self.n, 1)) if x0 is None else x0\n",
    "\n",
    "    def predict(self, u = 0):\n",
    "        self.x = np.dot(self.F, self.x) + np.dot(self.B, u)\n",
    "        self.P = np.dot(np.dot(self.F, self.P), self.F.T) + self.Q\n",
    "        return self.x\n",
    "\n",
    "    def update(self, z):\n",
    "        y = z - np.dot(self.H, self.x)\n",
    "        S = self.R + np.dot(self.H, np.dot(self.P, self.H.T))\n",
    "        K = np.dot(np.dot(self.P, self.H.T), np.linalg.inv(S))\n",
    "        self.x = self.x + np.dot(K, y)\n",
    "        I = np.eye(self.n)\n",
    "        self.P = np.dot(I - np.dot(K, self.H), self.P)\n",
    "\n",
    "def example():\n",
    "    dt = 1.0/60\n",
    "    F = np.array([[1, dt, 0], [0, 1, dt], [0, 0, 1]])\n",
    "    H = np.array([1, 0, 0]).reshape(1, 3)\n",
    "    Q = np.array([[0.05, 0.05, 0.0], [0.05, 0.05, 0.0], [0.0, 0.0, 0.0]])\n",
    "    R = np.array([1]).reshape(1, 1)\n",
    "    \n",
    "    itr  = 200\n",
    "    \n",
    "    def f(x):\n",
    "        return np.dot(F,x)+np.random.normal(0,5,3)\n",
    "    \n",
    "    real_state = []\n",
    "    x = np.array([0,0,0])\n",
    "    \n",
    "    for i in range(itr):\n",
    "        real_state.append(x[0])\n",
    "        x = f(x)\n",
    "    \n",
    "    measurements = [x-1+np.random.normal(0,1) for x in real_state]\n",
    "\n",
    "    kf = KalmanFilter(F = F, H = H, Q = Q, R = R)\n",
    "    predictions = []\n",
    "    for z in measurements:\n",
    "        predictions.append(kf.predict()[0])\n",
    "        kf.update(z)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(range(len(measurements)), measurements, label = 'Measurements')\n",
    "    plt.plot(range(len(predictions)), np.array(predictions), label = 'Kalman Filter Prediction')\n",
    "    plt.plot(range(len(real_state)), real_state, label = 'Real statement' )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13-complex optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## torch adam optimize complex parameter\n",
    "import torch as t\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "complex_param = t.tensor([1],dtype=t.complex64, requires_grad=True)\n",
    "\n",
    "# We will optimize on the mean squared error from a 1j\n",
    "target = 2.0\n",
    "\n",
    "def calc_loss(x):\n",
    "    return t.abs(x - target)**2\n",
    "\n",
    "optimizer = t.optim.AdamW([complex_param], lr=0.001)\n",
    "\n",
    "n = 10000\n",
    "loss_list = []\n",
    "values = t.zeros(n, dtype=t.complex64)\n",
    "for i in range(n):\n",
    "    optimizer.zero_grad()\n",
    "    loss = calc_loss(complex_param)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    values[i] = complex_param.detach()\n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(values.real, label='Real Part')\n",
    "plt.plot(values.imag, label='Imaginary Part')\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Complex Parameter')\n",
    "plt.title('Optimization Progress with as-implemented Adam')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optax complex parameter 优化\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "param = {'t':jnp.array(1.0, dtype=jnp.complex64)}\n",
    "tx = optax.adam(learning_rate=0.001)\n",
    "opt_state = tx.init(param)\n",
    "\n",
    "def loss(param):\n",
    "    x = param['t']\n",
    "    return jnp.sum(jnp.abs(x - 1j)**2)\n",
    "loss_grad = jax.jit(jax.value_and_grad(loss))\n",
    "\n",
    "@jax.jit\n",
    "def update_state(param, opt_state):\n",
    "    loss_val, grads = loss_grad(param)\n",
    "    updates, opt_state = tx.update(grads,opt_state)\n",
    "    param = optax.apply_updates(param, updates)\n",
    "    return param,opt_state, loss_val \n",
    "\n",
    "n=10000\n",
    "values=np.zeros(n, dtype=np.complex64)\n",
    "loss_list=np.zeros(n, dtype=np.complex64)\n",
    "for i in range(n):\n",
    "    param,opt_state, loss_val = update_state(param, opt_state)\n",
    "    loss_list[i] = loss_val\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14-nn.vmap on dynamic module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Foo(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train=True):\n",
    "        x = nn.Conv(3, kernel_size=(3, 3), strides=(1, 1), padding=\"SAME\")(x)\n",
    "        x = nn.BatchNorm(use_running_average=not train, axis_name=[\"b1\", \"b2\"])(\n",
    "            x\n",
    "        )  # I need add two extra batch dims, can we use a list of axis_name here?\n",
    "        x = nn.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Foo()\n",
    "var = net.init(key, jnp.ones([2,28,28,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    batch_stats: {\n",
       "        BatchNorm_0: {\n",
       "            mean: (3,),\n",
       "            var: (3,),\n",
       "        },\n",
       "    },\n",
       "    params: {\n",
       "        BatchNorm_0: {\n",
       "            bias: (3,),\n",
       "            scale: (3,),\n",
       "        },\n",
       "        Conv_0: {\n",
       "            bias: (3,),\n",
       "            kernel: (3, 3, 3, 3),\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_tree(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vars:\n",
      "FrozenDict({\n",
      "    batch_stats: {\n",
      "        BatchNorm_0: {\n",
      "            mean: (3,),\n",
      "            var: (3,),\n",
      "        },\n",
      "    },\n",
      "    params: {\n",
      "        BatchNorm_0: {\n",
      "            bias: (3,),\n",
      "            scale: (3,),\n",
      "        },\n",
      "        Conv_0: {\n",
      "            bias: (3,),\n",
      "            kernel: (3, 3, 3, 3),\n",
      "        },\n",
      "    },\n",
      "})\n",
      "train y\n",
      "(2, 2, 2, 28, 28, 3)\n",
      "infer_y shape:\n",
      "(2, 2, 2, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "\n",
    "@functools.partial(\n",
    "    nn.vmap,\n",
    "    variable_axes={\"params\": None, \"batch_stats\": None},\n",
    "    split_rngs={\"params\": False},\n",
    "    in_axes=(1, None),\n",
    "    out_axes=1,\n",
    "    axis_name=\"b2\",\n",
    ")\n",
    "@functools.partial(\n",
    "    nn.vmap,\n",
    "    variable_axes={\"params\": None, \"batch_stats\": None},\n",
    "    split_rngs={\"params\": False},\n",
    "    in_axes=(1, None),\n",
    "    out_axes=1,\n",
    "    axis_name=\"b1\",\n",
    ")\n",
    "class Foo(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train=True):\n",
    "        x = nn.Conv(3, kernel_size=(3, 3), strides=(1, 1), padding=\"SAME\")(x)\n",
    "        x = nn.BatchNorm(use_running_average=not train, axis_name=[\"b1\", \"b2\"])(\n",
    "            x\n",
    "        )  # I need add two extra batch dims, can we use a list of axis_name here?\n",
    "        x = nn.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "foo = Foo()\n",
    "x = jnp.ones(shape=(2, 2, 2, 28, 28, 3))\n",
    "rng = jax.random.PRNGKey(0)\n",
    "vars = foo.init({\"params\": rng}, x, True)\n",
    "print(\"vars:\")\n",
    "print(jax.tree_map(lambda x: x.shape, vars))\n",
    "\n",
    "train_y, new_batch_stats = foo.apply(\n",
    "    {\"params\": vars[\"params\"], \"batch_stats\": vars[\"batch_stats\"]},\n",
    "    x,\n",
    "    True, # the trick is use False, instead of train=True\n",
    "    mutable=[\"batch_stats\"],\n",
    ")\n",
    "print(\"train y\")\n",
    "print(train_y.shape)\n",
    "infer_y = foo.apply(\n",
    "    {\"params\": vars[\"params\"], \"batch_stats\": new_batch_stats[\"batch_stats\"]},\n",
    "    x,\n",
    "    False, # the trick is use False, instead of train=False\n",
    "    mutable=False,\n",
    ")\n",
    "print(\"infer_y shape:\")\n",
    "print(infer_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15-jax.vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial, wraps\n",
    "x = (0,1)\n",
    "y = {'a':3,'b':{'alpha':4, 'beta':5}}\n",
    "\n",
    "def expand(data):\n",
    "    return jax.tree_map(lambda x: jnp.ones([5,1])*x, data)\n",
    "\n",
    "x_ = expand(x)\n",
    "y_ = expand(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_dc(f):\n",
    "    @wraps(f)\n",
    "    def _f(*args, **kwargs):\n",
    "        import time\n",
    "        t0 = time.time()\n",
    "        y = f(*args, **kwargs)\n",
    "        t1 = time.time()\n",
    "        print(f'time cost = {t1-t0}')\n",
    "        return y\n",
    "    return _f\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.vmap, in_axes=(None, {'a':0, 'b':{'alpha':0,'beta':0}}), out_axes=(None,1))\n",
    "def f(x,y):\n",
    "    return x[0], {'aa':y['b']['alpha'], 'bb':y['b']['beta']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " {'aa': DeviceArray([[4., 4., 4., 4., 4.]], dtype=float32),\n",
       "  'bb': DeviceArray([[5., 5., 5., 5., 5.]], dtype=float32)})"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x,y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "X = jnp.ones(10)\n",
    "Y = jnp.arange(10)\n",
    "\n",
    "\n",
    "def poisson(alpha, beta):\n",
    "    lamb = alpha + beta*X\n",
    "    return jnp.where(lamb.any() < 0, 0, jnp.sum(Y*jnp.log(lamb) - jnp.log(lamb)))\n",
    "\n",
    "poisson_vmap = jax.jit(jax.vmap(poisson, in_axes=(-1,-1), out_axes=-1))\n",
    "alpha = jnp.ones([1,10])\n",
    "beta = jnp.ones([1,10])\n",
    "poisson_vmap(alpha,beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16-Module transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    mlp: {\n",
      "        hidden: {\n",
      "            bias: (3, 4),\n",
      "            kernel: (3, 4, 4),\n",
      "        },\n",
      "        out: {\n",
      "            bias: (3, 1),\n",
      "            kernel: (3, 4, 1),\n",
      "        },\n",
      "    },\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from jax import random, numpy as jnp\n",
    "from flax import linen as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  @nn.compact\n",
    "  def __call__(self, xs):\n",
    "    h = nn.Dense(4, name='hidden')(xs)\n",
    "    h = nn.relu(h)\n",
    "    return nn.Dense(1, name='out')(h)\n",
    "\n",
    "class ManualVmapMLP(nn.Module):\n",
    "  @nn.compact\n",
    "  def __call__(self, xs):\n",
    "    mlp = MLP(parent=None)\n",
    "    init_fn = lambda rng, xs: jax.vmap(mlp.init, in_axes=0)(random.split(rng, xs.shape[0]), xs)['params']\n",
    "    apply_fn = jax.vmap(mlp.apply, in_axes=0)\n",
    "    mlp_params = self.param('mlp', init_fn, xs)\n",
    "    return apply_fn({'params': mlp_params}, xs)\n",
    "\n",
    "xs = jnp.ones((3, 4))\n",
    "variables = ManualVmapMLP().init(random.PRNGKey(0), xs)\n",
    "print(jax.tree_map(jnp.shape, variables['params']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(x):\n",
    "    H = jnp.ones(len(x))\n",
    "    x = jnp.fft.ifft(jnp.fft.fft(x)*H)\n",
    "    x = jnp.exp(1j*jnp.abs(x)**2)\n",
    "    return x\n",
    "\n",
    "trans_vmap = jax.vmap(trans, in_axes=0, out_axes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17-convolution speed compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.6 ms ± 4.53 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "848 µs ± 130 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "1.8 ms ± 270 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "42.2 ms ± 11.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "The slowest run took 5.30 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "103 ms ± 58.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "from commplax.xop import convolve\n",
    "\n",
    "x = jnp.ones(10000)\n",
    "\n",
    "@jax.jit\n",
    "def jax_convolve(x, y):\n",
    "  return jax.scipy.signal.convolve(x,x,mode='same')\n",
    "\n",
    "## convolution speed\n",
    "\n",
    "# %timeit jnp.convolve(x,x)\n",
    "%timeit np.convolve(x,x)\n",
    "%timeit scipy.signal.convolve(x,x)\n",
    "%timeit convolve(x,x,method='auto')\n",
    "\n",
    "\n",
    "## matrix product speed\n",
    "vmap = jax.vmap\n",
    "vv = lambda x, y: jnp.vdot(x, y)  #  ([a], [a]) -> []\n",
    "mv = vmap(vv, (0, None), 0)      #  ([b,a], [a]) -> [b]      (b is the mapped axis)\n",
    "mm = vmap(mv, (None, 1), 1) \n",
    "mv2 = vmap(vv, (0, 1), 0) \n",
    "mm2 = vmap(mv2, (1, 1), 0)\n",
    "\n",
    "a,b,c = 1000,1000,1000\n",
    "\n",
    "%timeit jax.jit(mm)(np.random.rand(b,a), np.random.rand(a,c))\n",
    "%timeit np.random.rand(b,a) @ np.random.rand(a,c)\n",
    "\n",
    "\n",
    "## random nunber generation speed\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "%timeit jax.random.normal(key, (1000000,))\n",
    "%timeit np.random.normal(size=(1000000,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18-scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.numpy.fft import fft, ifft, fftfreq\n",
    "from optical_flax.models import edfa\n",
    "\n",
    "def one_step(Ech , _, *, hz=0.1, linOperator=None, γ=1.3):\n",
    "    # First linear step (frequency domain)\n",
    "    Ech = Ech * linOperator            \n",
    "\n",
    "    # Nonlinear step (time domain)\n",
    "    Ech = ifft(Ech)\n",
    "    Ech = Ech * jnp.exp(1j*γ*(Ech*jnp.conj(Ech))*hz)\n",
    "\n",
    "    # Second linear step (frequency domain)\n",
    "    Ech = fft(Ech)       \n",
    "    Ech = Ech * linOperator   \n",
    "    return Ech, None\n",
    "\n",
    "@partial(jax.jit, static_argnames=['amp'])\n",
    "def one_span(Ech, _, *, amp='efda',Fs= 1, Lspan=1, NF=4.5, Fc=1, hz=1, Nsteps=1, α=0.2):\n",
    "    Ech =  fft(Ech)\n",
    "    Ech = jax.lax.scan(one_step, x, None,  length=10)[0]\n",
    "    Ech = ifft(Ech)\n",
    "\n",
    "    if amp =='edfa':\n",
    "        Ech = edfa(Ech, Fs, alpha*Lspan, NF, Fc)\n",
    "    elif amp =='ideal':\n",
    "        Ech = Ech * jnp.exp(α/2*Nsteps*hz)\n",
    "    elif amp == None:\n",
    "        Ech = Ech * jnp.exp(0); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19-Generate random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit)\n",
    "def f(x, p):\n",
    "    return x + np.random.normal(size=x.size)*p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-0.29408526, -0.0387882 ,  2.7437122 ,  0.20193726,\n",
       "              1.0296832 ,  2.069316  ,  1.8907064 ,  2.7548862 ,\n",
       "              2.495644  ,  2.0693927 ], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(jnp.ones(10),1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "531552b898093c495d5d7fbfac81f0b68153c16134e4b180957a1aaadf98359a"
  },
  "kernelspec": {
   "display_name": "Python (comm)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
